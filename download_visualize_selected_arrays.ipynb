{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the selected datasets from NASA Earthdata"
      ],
      "metadata": {
        "id": "DW07KIONObbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cartopy earthaccess netCDF4 pydap"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8cs5qPuW1Uwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import re\n",
        "import tempfile\n",
        "import warnings\n",
        "from base64 import b64encode\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "from typing import List, Tuple, Any\n",
        "from urllib.parse import quote\n",
        "\n",
        "import earthaccess\n",
        "import google.colab\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import xarray as xr\n",
        "from pydap.client import open_url\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "metadata": {
        "id": "5vVeUyzl_Svj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDL token\n",
        "\n",
        "The easiest way to get authentication is to obtain an Earthdata Login token (EDL token) and use it for requests. One way of obtaining the EDL token is described in the section 3 of [this](https://github.com/nasa/gesdisc-tutorials/blob/main/notebooks/How_to_Generate_Earthdata_Prerequisite_Files.ipynb) jupyter notebook.\n",
        "\n",
        "You can call the function `save_edl_token_to_file()` below if you need to. You will have to specify the NASA Earthdata account login and password, so complete the registration procedure beforehand. The detailed manual can be found [here](https://disc.gsfc.nasa.gov/information/documents?title=Data%20Access)."
      ],
      "metadata": {
        "id": "9wTARXWzO6sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_edl_token_to_file(token_file_path: str = \"edl_token.txt\"):\n",
        "    # Earthdata Login URL for obtaining the token, and creating one if it doesn't exist\n",
        "    url = 'https://urs.earthdata.nasa.gov/api/users/find_or_create_token'\n",
        "\n",
        "    # Earthdata Login credential prompts\n",
        "    prompts = {\n",
        "        'username': 'Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
        "        'password': 'Enter NASA Earthdata Login Password: '\n",
        "    }\n",
        "\n",
        "    # Get credentials from user input\n",
        "    username = getpass(prompt=prompts['username'])\n",
        "    password = getpass(prompt=prompts['password'])\n",
        "\n",
        "    # Encode credentials using Base64\n",
        "    credentials = b64encode(f\"{username}:{password}\".encode('utf-8')).decode('utf-8')\n",
        "\n",
        "    # Headers with the Basic Authorization\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {credentials}'\n",
        "    }\n",
        "\n",
        "    # Make the POST request to get the token\n",
        "    response = requests.post(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the response JSON to get the token\n",
        "        token_info = response.json()\n",
        "        token = token_info.get(\"access_token\")\n",
        "        print(\"Token retrieved successfully\")\n",
        "\n",
        "        # Write the token to the .edl_token file\n",
        "        with open(token_file_path, 'w') as token_file:\n",
        "            token_file.write(token)\n",
        "\n",
        "        print(f\"Token saved to {token_file_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to retrieve token:\", response.text)\n"
      ],
      "metadata": {
        "id": "nMLOG_IUR_N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call `save_edl_token_to_file()` below if you need to"
      ],
      "metadata": {
        "id": "GAa8Li6dTvmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    save_edl_token_to_file()"
      ],
      "metadata": {
        "id": "LeKccqzpTUMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the token was saved to `edl_token.txt`, you can see the contents of the file by running `!cat edl_token.txt` in a code cell or by double-clicking on the file name in the file explorer."
      ],
      "metadata": {
        "id": "LyvYeAE9RSyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paste your EDL token (and check authentication)"
      ],
      "metadata": {
        "id": "H4lOtB8gT8nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'EARTHDATA_TOKEN' not in os.environ:\n",
        "    # paste your token securely (it won't show on-screen)\n",
        "    os.environ['EARTHDATA_TOKEN'] = getpass('Paste your EDL token: ')\n",
        "\n",
        "# initialize an authenticated session\n",
        "session = earthaccess.login(strategy='environment')   # uses EARTHDATA_TOKEN\n",
        "print('Authenticated?', session.authenticated)\n",
        "print()\n",
        "print(f\"{session=}\")\n",
        "print(f\"{session.get_session()=}\")"
      ],
      "metadata": {
        "id": "pA-10y6M1lCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the dates and region of interest"
      ],
      "metadata": {
        "id": "tDjlCCAeUPGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------------------\n",
        "# (!!!) SET THESE VARS MANUALLY:\n",
        "\n",
        "#Ladoga:\n",
        "LAT_MIN = 59.5\n",
        "LAT_MAX = 62.6\n",
        "LON_MIN = 28.0\n",
        "LON_MAX = 33.8\n",
        "\n",
        "DATE_MIN = '2025-11-01'\n",
        "DATE_MAX = '2025-11-02'\n",
        "\n",
        "PRODUCT = 'FS'\n",
        "OBSERVABLE_VARS = [\n",
        "    f'/{PRODUCT}/VER/sigmaZeroNPCorrected',\n",
        "]\n",
        "OBSERVABLE_NAME_TO_COLORBAR_TITLE = {\n",
        "    f'/{PRODUCT}/VER/sigmaZeroNPCorrected': \"NRCS Ku-band (dB)\"\n",
        "}\n",
        "SCAN_TIME_REQUIRED = False\n",
        "\n",
        "HDF5_BNAME_DELIMITER = '.'\n",
        "HDF5_BNAME_TRACK_NUMBER_PART_IDX = -2\n",
        "# ^ (-2) for 'GPM_2ADPR.07:2A.GPM.DPR.V9-20240130.20251101-S023933-E041248.066262.V07C.HDF5'\n",
        "HDF5_BNAME_TRACK_START_PART_IDX = -3\n",
        "# ^ (-3) for 'GPM_2ADPR.07:2A.GPM.DPR.V9-20240130.20251101-S023933-E041248.066262.V07C.HDF5'\n",
        "#----------------------------------------------------------------\n",
        "\n",
        "# These vars are inferred from the vars defined above\n",
        "requested_vars_slashes = [\n",
        "    f'/{PRODUCT}/Latitude',\n",
        "    f'/{PRODUCT}/Longitude',\n",
        "]\n",
        "if SCAN_TIME_REQUIRED:\n",
        "    requested_vars_slashes.extend([\n",
        "        f'/{PRODUCT}/ScanTime/Year',\n",
        "        f'/{PRODUCT}/ScanTime/Month',\n",
        "        f'/{PRODUCT}/ScanTime/DayOfMonth',\n",
        "        f'/{PRODUCT}/ScanTime/DayOfYear',\n",
        "        f'/{PRODUCT}/ScanTime/Hour',\n",
        "        f'/{PRODUCT}/ScanTime/Minute',\n",
        "        f'/{PRODUCT}/ScanTime/Second',\n",
        "        f'/{PRODUCT}/ScanTime/MilliSecond',\n",
        "    ])\n",
        "\n",
        "requested_vars_slashes.extend(OBSERVABLE_VARS)\n",
        "\n",
        "requested_vars_underscores = [v[1:].replace('/', '_') for v in requested_vars_slashes]"
      ],
      "metadata": {
        "id": "KG3o-XB7-vRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions (data search)\n",
        "\n",
        "Feel free to fold the code of each function if you are not going to modify the code."
      ],
      "metadata": {
        "id": "JFL489h8Ude9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_track_number_from_h5_url_or_fpath(h5_url_or_fpath: str) -> str:\n",
        "    fname = h5_url_or_fpath.split(\"/\")[-1]\n",
        "    bname = os.path.splitext(fname)[0]\n",
        "    parts = bname.split(HDF5_BNAME_DELIMITER)\n",
        "    track_number = parts[HDF5_BNAME_TRACK_NUMBER_PART_IDX]\n",
        "    return track_number\n",
        "\n",
        "def extract_track_start_timestamp_from_h5_url_or_fpath(h5_url_or_fpath: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the track start timestamp from an HDF5 filename or URL and return a datetime object.\n",
        "\n",
        "    Expected part format: \"<yyyy><mm><dd>-S<hh><mm><ss>-E<hh><mm><ss>\"\n",
        "    Example part: \"20251101-S023933-E041248\" -> returns datetime(2025,11,1,2,39,33)\n",
        "\n",
        "    Raises ValueError if the expected pattern cannot be found.\n",
        "    \"\"\"\n",
        "    fname = h5_url_or_fpath.split(\"/\")[-1]\n",
        "    bname = os.path.splitext(fname)[0]\n",
        "    parts = bname.split(HDF5_BNAME_DELIMITER)\n",
        "    track_timestamps = parts[HDF5_BNAME_TRACK_START_PART_IDX]\n",
        "\n",
        "    m = re.search(r'(?P<date>\\d{8})-S(?P<start>\\d{6})', track_timestamps)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Couldn't parse start timestamp from '{track_timestamps}' in '{h5_url_or_fpath}'\")\n",
        "\n",
        "    date = m.group('date')   # YYYYMMDD\n",
        "    start = m.group('start') # HHMMSS\n",
        "\n",
        "    year = int(date[0:4])\n",
        "    month = int(date[4:6])\n",
        "    day = int(date[6:8])\n",
        "\n",
        "    hour = int(start[0:2])\n",
        "    minute = int(start[2:4])\n",
        "    second = int(start[4:6])\n",
        "\n",
        "    return datetime(year, month, day, hour, minute, second)\n",
        "\n",
        "\n",
        "def get_search_data_results() -> list[str]:\n",
        "    temporal = (\n",
        "        f\"{DATE_MIN}T00:00:00Z\",\n",
        "        f\"{DATE_MAX}T23:59:59Z\"\n",
        "    )\n",
        "\n",
        "    results = earthaccess.search_data(\n",
        "        short_name=\"GPM_2ADPR\",\n",
        "        bounding_box=(LON_MIN, LAT_MIN, LON_MAX, LAT_MAX),\n",
        "        temporal=temporal,\n",
        "        count=-1,\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def find_opendap_url(granule_meta: dict) -> str | None:\n",
        "    \"\"\"Return the first OPeNDAP URL found in the granule UMM RelatedUrls (or None).\"\"\"\n",
        "    related = granule_meta.get(\"umm\", {}).get(\"RelatedUrls\", []) or []\n",
        "    for r in related:\n",
        "        url = r.get(\"URL\") or r.get(\"url\") or r.get(\"Href\") or r.get(\"Href\") if isinstance(r, dict) else None\n",
        "        if not url:\n",
        "            continue\n",
        "        if \"opendap\" in url.lower():\n",
        "            return url\n",
        "        # sometimes Type field holds the service type\n",
        "        if \"Type\" in r and ((\"opendap\" in (r.get(\"Type\") or \"\").lower()) or (\"OPeNDAP\" in (r.get(\"Type\") or \"\"))):\n",
        "            return url\n",
        "\n",
        "    # fallback: sometimes earthaccess.data_links() returns https download links only\n",
        "    # but many collections include OPENDAP in 'RelatedUrls' as above\n",
        "    return None\n",
        "\n",
        "\n",
        "def read_latlon_from_localfile(fobj) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Given a file-like object or path to a GPM HDF5 granule (opened by earthaccess.open),\n",
        "    save locally if necessary and use h5py to find latitude/longitude datasets.\n",
        "    Returns lat, lon (numpy arrays) and list of candidate data dataset paths found in file.\n",
        "    \"\"\"\n",
        "    # Ensure we have a filesystem path for h5py\n",
        "    if hasattr(fobj, \"name\") and isinstance(fobj.name, str):\n",
        "        local_path = fobj.name\n",
        "    else:\n",
        "        tmpf = tempfile.NamedTemporaryFile(suffix=\".HDF5\", delete=False)\n",
        "        tmp_path = tmpf.name\n",
        "        tmpf.write(fobj.read())\n",
        "        tmpf.close()\n",
        "        local_path = tmp_path\n",
        "\n",
        "    # open file with h5py\n",
        "    lat = lon = None\n",
        "    candidate_data_paths = []\n",
        "\n",
        "    with h5py.File(local_path, \"r\") as hf:\n",
        "        # walk file looking for dataset names that include 'lat' / 'lon' / 'latitude' / 'longitude'\n",
        "        def visitor(name, obj):\n",
        "            nonlocal lat, lon, candidate_data_paths\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                lower = name.lower()\n",
        "                if (\"latitude\" in lower) or (\"lat\" in lower and \"quality\" not in lower):\n",
        "                    # pick first reasonable-shaped lat dataset\n",
        "                    try:\n",
        "                        arr = obj[()]\n",
        "                    except Exception:\n",
        "                        arr = None\n",
        "                    if arr is not None and lat is None and arr.size > 0:\n",
        "                        lat = np.array(arr)\n",
        "                if (\"longitude\" in lower) or (\"lon\" in lower and \"quality\" not in lower):\n",
        "                    try:\n",
        "                        arr = obj[()]\n",
        "                    except Exception:\n",
        "                        arr = None\n",
        "                    if arr is not None and lon is None and arr.size > 0:\n",
        "                        lon = np.array(arr)\n",
        "                # heuristically collect candidate data variable paths (2D arrays)\n",
        "                if obj.ndim >= 2 and obj.size > 0 and \"latitude\" not in lower and \"longitude\" not in lower:\n",
        "                    candidate_data_paths.append(name)\n",
        "        hf.visititems(visitor)\n",
        "\n",
        "    return lat, lon, candidate_data_paths\n",
        "\n",
        "\n",
        "def indices_intersecting_bbox(\n",
        "    lat: np.ndarray,\n",
        "    lon: np.ndarray,\n",
        "    latmin: float,\n",
        "    latmax: float,\n",
        "    lonmin: float,\n",
        "    lonmax: float,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given lat and lon arrays (typically 2D: scans x pixels), return an array of scan indices\n",
        "    (0-based) that have at least one pixel inside the bbox.\n",
        "    \"\"\"\n",
        "    if lat is None or lon is None:\n",
        "        return np.array([], dtype=int)\n",
        "\n",
        "    # normalize shapes: if 1D vs 2D\n",
        "    if lat.ndim == 1 and lon.ndim == 1:\n",
        "        # 1D along scans\n",
        "        mask = (lat >= latmin) & (lat <= latmax) & (lon >= lonmin) & (lon <= lonmax)\n",
        "        return np.where(mask)[0]\n",
        "\n",
        "    if lat.ndim == 2 and lon.ndim == 2:\n",
        "        # assume axis 0 = scans, axis 1 = pixels OR vice versa; find which axis is longer (scans)\n",
        "        # better: treat axis 0 as scans if first axis length > second (typical)\n",
        "        if lat.shape[0] >= lat.shape[1]:\n",
        "            scan_axis = 0\n",
        "            pixel_axis = 1\n",
        "        else:\n",
        "            scan_axis = 1\n",
        "            pixel_axis = 0\n",
        "            # transpose to make scans axis 0\n",
        "            lat = lat.T\n",
        "            lon = lon.T\n",
        "\n",
        "        mask = (lat >= latmin) & (lat <= latmax) & (lon >= lonmin) & (lon <= lonmax)\n",
        "        # check any pixel in each scan\n",
        "        scan_mask = np.any(mask, axis=1)\n",
        "        return np.where(scan_mask)[0]\n",
        "\n",
        "    # fallback: try flatten and match approximate\n",
        "    mask = (lat >= latmin) & (lat <= latmax) & (lon >= lonmin) & (lon <= lonmax)\n",
        "    if mask.any():\n",
        "        return np.array([0])  # we can't identify scans reliably, return the file-level match\n",
        "\n",
        "    return np.array([], dtype=int)\n",
        "\n",
        "\n",
        "def contiguous_ranges(idxs: np.ndarray) -> List[Tuple[int,int]]:\n",
        "    \"\"\"Turn sorted 1D idx array into list of (start, end) inclusive ranges.\"\"\"\n",
        "    if idxs.size == 0:\n",
        "        return []\n",
        "    idxs = np.sort(idxs)\n",
        "    ranges = []\n",
        "    start = prev = idxs[0]\n",
        "    for i in idxs[1:]:\n",
        "        if i == prev + 1:\n",
        "            prev = i\n",
        "            continue\n",
        "        else:\n",
        "            ranges.append((start, prev))\n",
        "            start = prev = i\n",
        "    ranges.append((start, prev))\n",
        "    return ranges\n"
      ],
      "metadata": {
        "id": "smRbWY7Z4GG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the sliced arrays"
      ],
      "metadata": {
        "id": "6ulvEtUdUohn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_search_data_results()\n",
        "\n",
        "track_number_and_start_time = {}\n",
        "track_number_to_arr_dict = {}\n",
        "\n",
        "# main loop: for each granule found by your search\n",
        "for g in results:\n",
        "    title = g.get(\"meta\", {}).get(\"title\") or g.get(\"meta\", {}).get(\"granule_ur\") or g.get(\"umm\", {}).get(\"GranuleUR\")\n",
        "    print(\"\\n=== Granule:\", title)\n",
        "    opendap_base = find_opendap_url(g)\n",
        "    if not opendap_base:\n",
        "        print(\" No OPeNDAP URL found in RelatedUrls for this granule; skipping.\")\n",
        "        continue\n",
        "    print(\" OPeNDAP URL (base):\", opendap_base)\n",
        "\n",
        "    fileobjs = earthaccess.open([g])\n",
        "    fobj = fileobjs[0]\n",
        "    # open the granule once with earthaccess (handles auth for you)\n",
        "    try:\n",
        "        # read lat/lon and candidate data paths\n",
        "        lat, lon, candidate_data_paths = read_latlon_from_localfile(fobj)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Failed to open granule with earthaccess.open(): {e}\")\n",
        "        lat = lon = None\n",
        "        candidate_data_paths = []\n",
        "\n",
        "    if lat is None or lon is None:\n",
        "        print(\" Could not locate lat/lon arrays inside file. Candidate data paths (heuristic):\")\n",
        "        for p in candidate_data_paths[:10]:\n",
        "            print(\"  -\", p)\n",
        "        print(\" You may need to inspect file structure manually. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # find scan indices that intersect your box\n",
        "    idxs = indices_intersecting_bbox(lat, lon, LAT_MIN, LAT_MAX, LON_MIN, LON_MAX)\n",
        "    print(\" Scan indices intersecting ROI (count):\", idxs.size)\n",
        "    if idxs.size == 0:\n",
        "        print(\" No scans in this granule intersect the bounding box.\")\n",
        "        continue\n",
        "\n",
        "    ranges = contiguous_ranges(idxs)\n",
        "    #print(\" Contiguous scan ranges (inclusive):\", ranges)\n",
        "    single_enclosing_range = (\n",
        "        min(left for left, right in ranges),\n",
        "        max(right for left, right in ranges)\n",
        "    )\n",
        "    print(\" Scan range (inclusive):\", single_enclosing_range)\n",
        "\n",
        "    # compute pixel range (use full pixel range)\n",
        "    if lat.ndim == 2:\n",
        "        # ensure we computed scans as axis 0 above\n",
        "        n_pixels = lat.shape[1] if lat.shape[0] >= lat.shape[1] else lat.shape[0]\n",
        "    else:\n",
        "        n_pixels = 0\n",
        "\n",
        "    # pick variables to subset: use requested_vars_slashes if provided, otherwise pick top 2 candidate_data_paths\n",
        "    if requested_vars_slashes:\n",
        "        vars_to_request = requested_vars_slashes\n",
        "    else:\n",
        "        # prefer candidate paths that are clearly data (not quality flags)\n",
        "        filtered = [p for p in candidate_data_paths if all(x not in p.lower() for x in (\"qual\",\"status\",\"flag\",\"count\"))]\n",
        "        if not filtered:\n",
        "            filtered = candidate_data_paths\n",
        "        # take first 3 candidates to request by default\n",
        "        vars_to_request = filtered[:3]\n",
        "\n",
        "    print(\" Will request variables:\", vars_to_request)\n",
        "\n",
        "    # For each contiguous range, build constraint expression and attempt to open via xarray+pydap\n",
        "    sidx, eidx = [int(idx) for idx in single_enclosing_range]\n",
        "\n",
        "    subset_url_dap4 = opendap_base.replace(\"https://\", \"dap4://\")\n",
        "    print(\"subset_url_dap4 (trying):\", subset_url_dap4)\n",
        "\n",
        "    try:\n",
        "        ds_pydap = open_url(subset_url_dap4, session=session.get_session())\n",
        "        print(\" Successfully called `open_url(subset_url_dap4, session=session.get_session())`.\")\n",
        "        print(\" Variables:\", list(ds_pydap.keys()))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"xarray/pydap failed to open subset_url: {e}\")\n",
        "        print(\" You can either: (1) inspect the exact variable names in the OPeNDAP DDS/DAS,\")\n",
        "        print(\" or (2) download the granule locally and slice after loading with h5py/xarray.\")\n",
        "        # continue to next range\n",
        "        continue\n",
        "\n",
        "    selected_np_arrays = {}\n",
        "    for var_name_slashes in requested_vars_slashes:\n",
        "        var_name_underscores = var_name_slashes[1:].replace('/', '_')\n",
        "        arr_obj = (\n",
        "            ds_pydap[var_name_underscores][sidx:eidx]\n",
        "            if n_pixels > 0\n",
        "            else ds_pydap[var_name_underscores]\n",
        "        )\n",
        "        selected_np_arrays[var_name_slashes] = np.array(arr_obj)\n",
        "\n",
        "    track_number = extract_track_number_from_h5_url_or_fpath(title)\n",
        "    track_number_to_arr_dict[track_number] = selected_np_arrays\n",
        "\n",
        "    track_number_and_start_time[title] = (\n",
        "        track_number,\n",
        "        extract_track_start_timestamp_from_h5_url_or_fpath(title).isoformat().replace(\"T\", \" \")\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "cK_z3tLuQP4L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Granule name -> (track_number, track_start_datetime)\")\n",
        "pp.pprint(track_number_and_start_time)\n",
        "\n",
        "print()\n",
        "print(\"Structure of the obtained data (track numbers, variable names, numpy array dtypes and shapes):\")\n",
        "pp.pprint(\n",
        "    {\n",
        "        track_number: {k: (v.dtype, v.shape) for k, v in arr_dict.items()}\n",
        "        for track_number, arr_dict in track_number_to_arr_dict.items()\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "3oJfNVTHy3O4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the arrays"
      ],
      "metadata": {
        "id": "QCqekXWlOKJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_to_save = {\n",
        "    granule_name: track_number_to_arr_dict[track_number]\n",
        "    for granule_name, (track_number, track_start_time) in track_number_and_start_time.items()\n",
        "}\n",
        "\n",
        "if False:\n",
        "    pp.pprint(\n",
        "        {\n",
        "            granule_name: {k: (v.dtype, v.shape) for k, v in arr_dict.items()}\n",
        "            for granule_name, arr_dict in dict_to_save.items()\n",
        "        }\n",
        "    )\n",
        "\n",
        "np.savez_compressed(\n",
        "    f\"./from_{DATE_MIN}_to_{DATE_MAX}\",\n",
        "    **dict_to_save,\n",
        ")"
      ],
      "metadata": {
        "id": "1MrolGdnOJiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "4m2fnYwORjWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the arrays"
      ],
      "metadata": {
        "id": "hbCAOq39X-Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import ColumnDataSource, LabelSet, Range1d, ColorBar, HoverTool, LinearColorMapper\n",
        "from bokeh.palettes import Turbo256\n",
        "from bokeh.transform import transform\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from shapely.geometry import LineString, MultiLineString, MultiPolygon, Polygon\n",
        "\n",
        "output_notebook()"
      ],
      "metadata": {
        "id": "t1IvCbgbXgx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------------------\n",
        "# (!!!) SET THESE VARS MANUALLY:\n",
        "\n",
        "SINGLE_SELECTED_OBSERVABLE_VAR = f'/{PRODUCT}/VER/sigmaZeroNPCorrected'\n",
        "SLICE_FOR_SELECTED_OBSERVABLE_VAR = (slice(None), slice(None), 0)\n",
        "# ^ (slice(None), slice(None), 0) corresponds to the [:, :, 0] slicing\n",
        "\n",
        "SHOW_RIVERS = True\n",
        "\n",
        "ADD_HOVER_TOOL = False\n",
        "BOKEH_FIGURE_MAX_SIZE = 800\n",
        "BOKEH_FIGURE_MARKER_SIZE = 6\n",
        "#----------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "pMXOl_iTVTYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions (data visualization using bokeh)"
      ],
      "metadata": {
        "id": "s7JAPKbzTEZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_bokeh_figure(\n",
        "    plot_title: str,\n",
        ") -> figure:\n",
        "    # Extract bounding box coordinates\n",
        "    lon_min, lon_max = LON_MIN, LON_MAX\n",
        "    lat_min, lat_max = LAT_MIN, LAT_MAX\n",
        "\n",
        "    try:\n",
        "        height_to_width_ratio = (lat_max - lat_min) / (lon_max - lon_min)\n",
        "    except ZeroDivisionError:\n",
        "        height_to_width_ratio = 1.0\n",
        "\n",
        "    if height_to_width_ratio > 1.0:\n",
        "        fig_height = BOKEH_FIGURE_MAX_SIZE\n",
        "        fig_width = int(fig_height / height_to_width_ratio)\n",
        "    else:\n",
        "        fig_width = BOKEH_FIGURE_MAX_SIZE\n",
        "        fig_height = int(fig_width * height_to_width_ratio)\n",
        "\n",
        "    # Create Bokeh figure\n",
        "    p = figure(\n",
        "        title=plot_title,\n",
        "        x_range=Range1d(lon_min, lon_max, bounds=(lon_min, lon_max)),\n",
        "        y_range=Range1d(lat_min, lat_max, bounds=(lat_min, lat_max)),\n",
        "        width=fig_width,\n",
        "        height=fig_height,\n",
        "        tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
        "        toolbar_location=\"left\",\n",
        "    )\n",
        "\n",
        "    # Configure plot appearance\n",
        "    p.title.text_font_size = \"16pt\"\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def get_geojson_source(feature):\n",
        "    \"\"\"Convert cartopy feature to Bokeh ColumnDataSource\"\"\"\n",
        "    xs, ys = [], []\n",
        "\n",
        "    for geom in feature.geometries():\n",
        "        if geom.is_empty:\n",
        "            continue\n",
        "\n",
        "        # Normalize Multi* into parts\n",
        "        parts = getattr(geom, \"geoms\", [geom])\n",
        "        for g in parts:\n",
        "            gt = g.geom_type\n",
        "\n",
        "            if gt in (\"LineString\", \"LinearRing\"):\n",
        "                x, y = g.xy\n",
        "                xs.append(list(x))\n",
        "                ys.append(list(y))\n",
        "\n",
        "            elif gt == \"MultiLineString\":\n",
        "                for line in g.geoms:\n",
        "                    x, y = line.xy\n",
        "                    xs.append(list(x))\n",
        "                    ys.append(list(y))\n",
        "\n",
        "            elif gt == \"Polygon\":\n",
        "                # outline (exterior)\n",
        "                x, y = g.exterior.xy\n",
        "                xs.append(list(x))\n",
        "                ys.append(list(y))\n",
        "                # (optional) holes\n",
        "                # for ring in g.interiors:\n",
        "                #     rx, ry = ring.xy\n",
        "                #     xs.append(list(rx))\n",
        "                #     ys.append(list(ry))\n",
        "\n",
        "            elif gt == \"MultiPolygon\":\n",
        "                for poly in g.geoms:\n",
        "                    x, y = poly.exterior.xy\n",
        "                    xs.append(list(x))\n",
        "                    ys.append(list(y))\n",
        "                    # (optional) holes\n",
        "                    # for ring in poly.interiors:\n",
        "                    #     rx, ry = ring.xy\n",
        "                    #     xs.append(list(rx))\n",
        "                    #     ys.append(list(ry))\n",
        "\n",
        "    return ColumnDataSource(dict(xs=xs, ys=ys))\n",
        "\n",
        "\n",
        "def get_land_polygons():\n",
        "    \"\"\"Extract land polygons from Natural Earth features\"\"\"\n",
        "    land_geoms = cfeature.NaturalEarthFeature(\"physical\", \"land\", \"50m\")\n",
        "    polygons = []\n",
        "\n",
        "    for geom in land_geoms.geometries():\n",
        "        if isinstance(geom, Polygon):\n",
        "            polygons.append(geom)\n",
        "        elif isinstance(geom, MultiPolygon):\n",
        "            polygons.extend(geom.geoms)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "\n",
        "def get_polygon_source(polygons, projection):\n",
        "    \"\"\"Convert polygons to Bokeh ColumnDataSource\"\"\"\n",
        "    xs, ys = [], []\n",
        "\n",
        "    for polygon in polygons:\n",
        "        # Extract exterior coordinates\n",
        "        x, y = polygon.exterior.xy\n",
        "        # Project coordinates if needed\n",
        "        if projection:\n",
        "            x, y = projection.transform_points(\n",
        "                ccrs.PlateCarree(), np.array(x), np.array(y)\n",
        "            )[:, :2].T\n",
        "        xs.append(x.tolist())\n",
        "        ys.append(y.tolist())\n",
        "\n",
        "    return ColumnDataSource(data=dict(xs=xs, ys=ys))\n",
        "\n",
        "\n",
        "def draw_earth_features(\n",
        "    p: figure,\n",
        ") -> figure:\n",
        "    water_related_geoms = cfeature.NaturalEarthFeature(\"physical\", \"coastline\", \"50m\")\n",
        "    water_related_source = get_geojson_source(water_related_geoms)\n",
        "    p.multi_line(\n",
        "        xs=\"xs\", ys=\"ys\",\n",
        "        source=water_related_source,\n",
        "        line_color=\"black\",\n",
        "        line_width=1,\n",
        "    )\n",
        "\n",
        "    water_related_geoms = cfeature.NaturalEarthFeature(\"physical\", \"lakes\", \"10m\")\n",
        "    water_related_source = get_geojson_source(water_related_geoms)\n",
        "    p.patches(\n",
        "        xs=\"xs\", ys=\"ys\",\n",
        "        source=water_related_source,\n",
        "        fill_alpha=0.0,\n",
        "        line_color=\"black\",\n",
        "        line_width=1,\n",
        "    )\n",
        "\n",
        "    if SHOW_RIVERS:\n",
        "        water_related_geoms = cfeature.NaturalEarthFeature(\"physical\", \"rivers_lake_centerlines\", \"10m\")\n",
        "        water_related_source = get_geojson_source(water_related_geoms)\n",
        "        p.multi_line(\n",
        "            xs=\"xs\", ys=\"ys\",\n",
        "            source=water_related_source,\n",
        "            line_color=\"blue\",\n",
        "            line_width=1.5,\n",
        "            line_cap=\"round\",\n",
        "            line_join=\"round\",\n",
        "        )\n",
        "\n",
        "    land_polygons = get_land_polygons()\n",
        "    land_source = get_polygon_source(land_polygons, projection=None)\n",
        "    p.patches(\n",
        "        xs=\"xs\",\n",
        "        ys=\"ys\",\n",
        "        source=land_source,\n",
        "        fill_color=\"#E0E0E0\",  # Light gray\n",
        "        fill_alpha=0.0,\n",
        "        line_color=\"black\",  # Outline color\n",
        "        line_width=0.5,  # Outline thickness\n",
        "    )\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def draw_points_colorbar(\n",
        "    p: Any,\n",
        "    source: ColumnDataSource,\n",
        "    observable: np.ndarray,\n",
        "):\n",
        "    color_mapper = LinearColorMapper(\n",
        "        palette=Turbo256,\n",
        "        low=min(observable),\n",
        "        high=max(observable),\n",
        "    )\n",
        "\n",
        "    p.scatter(\n",
        "        \"longitude\",\n",
        "        \"latitude\",\n",
        "        source=source,\n",
        "        marker=\"circle\",\n",
        "        size=BOKEH_FIGURE_MARKER_SIZE,\n",
        "        fill_color=transform(\"observable\", color_mapper),\n",
        "        fill_alpha=1.0, #0.5,\n",
        "        line_color=None,\n",
        "    )\n",
        "\n",
        "    observable_print_name = OBSERVABLE_NAME_TO_COLORBAR_TITLE[OBSERVABLE_VARS[0]]\n",
        "\n",
        "    if ADD_HOVER_TOOL:\n",
        "        tooltips = [\n",
        "            (\"(lat, lon)\", \"(@latitude{0.000}, @longitude{0.000})\"),\n",
        "            (observable_print_name, \"@observable{0.00}\"),\n",
        "        ]\n",
        "        hover = HoverTool(tooltips=tooltips)\n",
        "        p.add_tools(hover)\n",
        "\n",
        "    color_bar = ColorBar(\n",
        "        color_mapper=color_mapper,\n",
        "        label_standoff=12,\n",
        "        width=8,\n",
        "        location=(0, 0),\n",
        "        title=observable_print_name,\n",
        "        title_text_font_size=\"16pt\",\n",
        "        title_text_font_style=\"normal\",\n",
        "        major_label_text_font_size=\"14pt\",\n",
        "    )\n",
        "    p.add_layout(color_bar, \"right\")\n",
        "\n",
        "\n",
        "def visualize_single_track(\n",
        "    track_order_number: int,\n",
        "    track_number: str,\n",
        "    track_number_to_arr_dict: dict[str, str],\n",
        "    track_number_and_start_time: dict[str, tuple[str, str]],\n",
        "):\n",
        "    track_number_to_start_timestamp = {\n",
        "        track_number: start_timestamp\n",
        "        for granule_name, (track_number, start_timestamp) in track_number_and_start_time.items()\n",
        "    }\n",
        "    p = prepare_bokeh_figure(\n",
        "        f\"({track_order_number}) Track number {track_number}\"\n",
        "        f\"\\nTrack started {track_number_to_start_timestamp[track_number]}\",\n",
        "    )\n",
        "\n",
        "    arr_dict = track_number_to_arr_dict[track_number]\n",
        "    latitude_key = [var_name for var_name in requested_vars_slashes if 'latitude' in var_name.lower()][0]\n",
        "    latitude = arr_dict[latitude_key]\n",
        "    longitude_key = [var_name for var_name in requested_vars_slashes if 'longitude' in var_name.lower()][0]\n",
        "    longitude = arr_dict[longitude_key]\n",
        "\n",
        "    observable = arr_dict[SINGLE_SELECTED_OBSERVABLE_VAR][SLICE_FOR_SELECTED_OBSERVABLE_VAR].flatten()\n",
        "\n",
        "    source = ColumnDataSource(\n",
        "        data=dict(\n",
        "            latitude=latitude.flatten(),\n",
        "            longitude=longitude.flatten(),\n",
        "            observable=observable,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    draw_points_colorbar(\n",
        "        p,\n",
        "        source,\n",
        "        observable,\n",
        "    )\n",
        "\n",
        "    p = draw_earth_features(p)\n",
        "\n",
        "    show(p)\n"
      ],
      "metadata": {
        "id": "kj0EozNmTJu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#google.colab.output.no_vertical_scroll()\n",
        "\n",
        "for track_order_number, (track_number, _) in enumerate(track_number_to_arr_dict.items(), start=1):\n",
        "    visualize_single_track(\n",
        "        track_order_number,\n",
        "        track_number,\n",
        "        track_number_to_arr_dict,\n",
        "        track_number_and_start_time,\n",
        "    )\n",
        "\n",
        "# It is probably a good idea to click\n",
        "# 'View output fullscreen' in the 'Code cell output actions' menu"
      ],
      "metadata": {
        "id": "htfHnue8Zo1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "In-ia3iNRK5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}